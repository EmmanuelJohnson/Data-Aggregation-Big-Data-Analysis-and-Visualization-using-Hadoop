{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer as ss\n",
    "from nltk.stem import WordNetLemmatizer as wn\n",
    "#from nltk.stem import LancasterStemmer as ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(fileName, content):\n",
    "    file = open(fileName,\"w\") \n",
    "    file.write(content)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnConfig = {\n",
    "    'golf': 'g',\n",
    "    'baseball': 'bsb',\n",
    "    'basketball': 'bkb',\n",
    "    'football': 'fb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccConfig = {\n",
    "    \"mlb\": \"baseball\",\n",
    "    \"nfl\": \"football\",\n",
    "    \"nba\": \"basketball\",\n",
    "    \"golf\": \"golf\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nytMain(query, folder):\n",
    "    #query = \"golf\" or \"baseball\" or \"basketball\" or \"football\"\n",
    "    df = pd.read_csv(\"urls/\"+query+\"-nyt-links.csv\")\n",
    "    #Get all the related urls for the given query\n",
    "    result_urls = df.URL\n",
    "    print(\"Total New York Times URLs : \"+str(len(result_urls)))\n",
    "    #Scrape the content of the urls recieved using nyt api\n",
    "    counter = 0\n",
    "    for url in result_urls:\n",
    "        fileName = folder+\"/nyt/\"+query+\"/\"+str(counter)+\".txt\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            parser = BeautifulSoup(response.content, 'html.parser')\n",
    "            #Search for section tag with name attribute as articleBody\n",
    "            article = parser.find(\"section\", {\"name\":\"articleBody\"})\n",
    "            if article:\n",
    "                #Get all the p tag texts\n",
    "                paras = article.find_all(\"p\")\n",
    "                if len(paras) > 0:\n",
    "                    content = \"\"\n",
    "                    for p in paras:\n",
    "                        content += str(p.text.encode('utf-8').strip(), 'utf-8')\n",
    "                        content += \"\\n\"\n",
    "                    saveFile(fileName, content)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total New York Times URLs : 634\n",
      "Total New York Times URLs : 531\n",
      "Total New York Times URLs : 750\n"
     ]
    }
   ],
   "source": [
    "nytMain(\"football\", \"new_data\")\n",
    "nytMain(\"baseball\", \"new_data\")\n",
    "nytMain(\"basketball\", \"new_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCCUrls(domain, index_list):\n",
    "    record_list = []\n",
    "    for index in index_list:\n",
    "        print(\"Current Index : \" + index)\n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-\"+index+\"-index?url=\"+domain+\"&output=json\"\n",
    "        print(\"Current URL : \" + cc_url)\n",
    "        response = requests.get(cc_url)\n",
    "        if response.status_code == 200:\n",
    "            records = response.content.splitlines()\n",
    "            for record in records:\n",
    "                record_list.append(json.loads(record))\n",
    "    print(\"# Records Found : \" + str(len(record_list)))\n",
    "    return record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtmlDoc(record):\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "    raw_data = BytesIO(resp.content)\n",
    "    f = gzip.GzipFile(fileobj=raw_data)\n",
    "    data = f.read()\n",
    "    response = \"\"\n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().decode(encoding='utf-8', errors='strict').split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccMain(query, folder, domains):\n",
    "    index_list = [\"2019-04\",\"2019-09\",\"2019-13\"]\n",
    "    record_list = list()\n",
    "    for domain in domains:\n",
    "        record_list += getCCUrls(domain, index_list)\n",
    "    print(\"Total Common Crawl URLs : \", len(record_list))\n",
    "    result_urls = list()\n",
    "    temp = list()\n",
    "    counter = 0\n",
    "    for record in record_list:\n",
    "        fileName = folder+\"/cc/\"+ccConfig[query]+\"/\"+str(counter)+\".txt\"\n",
    "        url = urlparse(record['url'])\n",
    "        if url.scheme == \"http\":\n",
    "            continue\n",
    "        urlString = url.geturl()\n",
    "        strippedUrl = urlString[:urlString.find('?')]\n",
    "        urlPath = url.path\n",
    "        if urlPath not in temp:\n",
    "            result_urls.append(strippedUrl)\n",
    "            temp.append(urlPath)\n",
    "            html_content = getHtmlDoc(record)\n",
    "            parser = BeautifulSoup(html_content)\n",
    "            article = parser.find(\"div\", {\"itemprop\":\"articleBody\"})\n",
    "            if article:\n",
    "                paras = article.find_all(\"p\", {\"class\":\"p-text\"})\n",
    "                if len(paras) > 0:\n",
    "                    content = \"\"\n",
    "                    for p in paras:\n",
    "                        content += str(p.text.encode('utf-8').strip(), 'utf-8')\n",
    "                        content += \"\\n\"\n",
    "                    saveFile(fileName, content)\n",
    "        counter += 1\n",
    "    print(len(result_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/golf/2019/01/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/golf/2019/01/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/golf/2019/01/*&output=json\n",
      "# Records Found : 96\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/golf/2019/02/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/golf/2019/02/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/golf/2019/02/*&output=json\n",
      "# Records Found : 104\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/golf/2019/03/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/golf/2019/03/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/golf/2019/03/*&output=json\n",
      "# Records Found : 49\n",
      "Total Common Crawl URLs :  249\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "cQuery = \"golf\"\n",
    "domains = [\"usatoday.com/story/sports/\"+cQuery+\"/2019/01/*\",\"usatoday.com/story/sports/\"+cQuery+\"/2019/02/*\",\n",
    "           \"usatoday.com/story/sports/\"+cQuery+\"/2019/03/*\"]\n",
    "ccMain(cQuery, \"new_data\", domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/nfl/2019/01/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/nfl/2019/01/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/nfl/2019/01/*&output=json\n",
      "# Records Found : 309\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/nfl/2019/02/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/nfl/2019/02/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/nfl/2019/02/*&output=json\n",
      "# Records Found : 101\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/nfl/2019/03/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/nfl/2019/03/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/nfl/2019/03/*&output=json\n",
      "# Records Found : 58\n",
      "Total Common Crawl URLs :  468\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "cQuery = \"nfl\"\n",
    "domains = [\"usatoday.com/story/sports/\"+cQuery+\"/2019/01/*\",\"usatoday.com/story/sports/\"+cQuery+\"/2019/02/*\",\n",
    "           \"usatoday.com/story/sports/\"+cQuery+\"/2019/03/*\"]\n",
    "ccMain(cQuery, \"new_data\", domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/mlb/2019/01/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/mlb/2019/01/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/mlb/2019/01/*&output=json\n",
      "# Records Found : 215\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/mlb/2019/02/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/mlb/2019/02/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/mlb/2019/02/*&output=json\n",
      "# Records Found : 87\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/mlb/2019/03/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/mlb/2019/03/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/mlb/2019/03/*&output=json\n",
      "# Records Found : 24\n",
      "Total Common Crawl URLs :  326\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "cQuery = \"mlb\"\n",
    "domains = [\"usatoday.com/story/sports/\"+cQuery+\"/2019/01/*\",\"usatoday.com/story/sports/\"+cQuery+\"/2019/02/*\",\n",
    "           \"usatoday.com/story/sports/\"+cQuery+\"/2019/03/*\"]\n",
    "ccMain(cQuery, \"new_data\", domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/nba/2019/01/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/nba/2019/01/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/nba/2019/01/*&output=json\n",
      "# Records Found : 150\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/nba/2019/02/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/nba/2019/02/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/nba/2019/02/*&output=json\n",
      "# Records Found : 74\n",
      "Current Index : 2019-04\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-04-index?url=usatoday.com/story/sports/nba/2019/03/*&output=json\n",
      "Current Index : 2019-09\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-09-index?url=usatoday.com/story/sports/nba/2019/03/*&output=json\n",
      "Current Index : 2019-13\n",
      "Current URL : http://index.commoncrawl.org/CC-MAIN-2019-13-index?url=usatoday.com/story/sports/nba/2019/03/*&output=json\n",
      "# Records Found : 36\n",
      "Total Common Crawl URLs :  260\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "cQuery = \"nba\"\n",
    "domains = [\"usatoday.com/story/sports/\"+cQuery+\"/2019/01/*\",\"usatoday.com/story/sports/\"+cQuery+\"/2019/02/*\",\n",
    "           \"usatoday.com/story/sports/\"+cQuery+\"/2019/03/*\"]\n",
    "ccMain(cQuery, \"new_data\", domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemSentence(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    final = \"\"\n",
    "#     sb = ss(\"english\")\n",
    "    wnl = wn()\n",
    "    for w in words: \n",
    "#         root = sb.stem(w)\n",
    "        root = wnl.lemmatize(w)\n",
    "        final += root\n",
    "        final += \" \"\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processArticles(files, dest, fn):\n",
    "    counter = 0\n",
    "    for f in files:\n",
    "        sample = open(f,\"r\")\n",
    "        refinedFinal = \"\"\n",
    "        print(\"Processing File # {}\".format(counter), end=\"\\r\", flush=True)\n",
    "        for line in sample.readlines():\n",
    "            lowerLine = line.lower()\n",
    "            noUrls = re.sub(r\"http\\S+\", \"\", lowerLine)\n",
    "            noUnderscore = noUrls.replace(\"_\", \"\")\n",
    "            #remove digits\n",
    "            noNoLine = re.sub(r'\\d+', '', noUnderscore)\n",
    "            #remove punctuations\n",
    "            words = re.findall(r'\\w+', noNoLine, flags = re.UNICODE)# | re.LOCALE\n",
    "            #remove stop words\n",
    "            important_words = filter(lambda x: x not in stopwords.words('english') and x.isdigit() == False and x not in letters, words)\n",
    "            refined = \" \".join(important_words)\n",
    "            #Get root words for the given words\n",
    "            refinedFinal += stemSentence(refined)\n",
    "            refinedFinal += \"\\n\"\n",
    "        saveFile(dest+fn+str(counter)+\".txt\", refinedFinal)\n",
    "        counter += 1\n",
    "        sample.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweets(csvFile, dest):\n",
    "    fn = csvFile.split(\".csv\")[0].split(\"/\")[-1]\n",
    "    df = pd.read_csv(csvFile)\n",
    "    tweets = df.tweet_text\n",
    "    refinedFinal = \"\"\n",
    "    count = 0\n",
    "    tempList = list()\n",
    "    for tweet in tweets:\n",
    "        print(\"Processing Tweet # {}\".format(count), end=\"\\r\", flush=True)\n",
    "        lowerLine = tweet.lower()\n",
    "        noUrls = re.sub(r\"http\\S+\", \"\", lowerLine)\n",
    "        noUnderscore = noUrls.replace(\"_\", \"\")\n",
    "        #remove digits\n",
    "        noNoLine = re.sub(r'\\d+', '', noUnderscore)\n",
    "        #remove punctuations\n",
    "        words = re.findall(r'\\w+', noNoLine, flags = re.UNICODE)# | re.LOCALE\n",
    "        #remove stop words\n",
    "        important_words = filter(lambda x: x not in stopwords.words('english') and x.isdigit() == False and x not in letters, words)\n",
    "        refined = \" \".join(important_words)\n",
    "        #Get root words for the given words\n",
    "        refinedStemmed = stemSentence(refined)\n",
    "        if refinedStemmed in tempList:\n",
    "            continue\n",
    "        tempList.append(refinedStemmed)\n",
    "        refinedFinal += refinedStemmed\n",
    "        refinedFinal += \"\\n\"\n",
    "        count += 1\n",
    "    saveFile(dest+fn+\".txt\", refinedFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataMain(directory, dest, ctype):\n",
    "    if ctype == \"twitter\":\n",
    "        processTweets(directory, dest)\n",
    "    else:\n",
    "        files=glob.glob(directory+\"*.txt\")\n",
    "        fn = ctype+\"_\"+fnConfig[directory.split(\"/\")[-2]]+\"_\"\n",
    "        processArticles(files, dest, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File # 140\r"
     ]
    }
   ],
   "source": [
    "articleDir = \"new_data/cc/golf/\"\n",
    "aDest = \"data/cc/\"\n",
    "processDataMain(articleDir, aDest, \"cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
